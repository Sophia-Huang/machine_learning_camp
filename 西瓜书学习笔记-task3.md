# 决策树

决策树有三种常见的算法：ID3、C4.5、CART。这三种模型的主要区别是分裂节点选择的指标不同。

ID3使用信息增益。

- 熵$H(D)=\sum\limits_{x \in D}-p\log p$：表示随机变量的不确定性。

- 条件熵$H(D|A)$：在一个条件下，随机变量的不确定性。

- 信息增益$g(D,A)=H(D)-H(D|A)$：熵 - 条件熵。表示在一个条件下，信息不确定性减少的程度。 

C4.5使用信息增益率。

- 数据集D关于特征A的值的熵$H_A(D)=-\sum\limits_{i=1}^n\frac{|D_i|}{|D|}\log \frac{|D_i|}{|D|}$ 

- 信息增益率$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$

CART使用基尼系数。

- 基尼系数$G(D)=1-\sum\limits_{x \in D}p^2$
- 条件基尼系数$G(D,A)=\sum\limits_{i=1}^n\frac{|D_i|}{|D|}G(D_i)$ 

# 回归决策树

决策树模型的基本思路是按照特征的取值将特征空间划分为一系列简单的区域，每一个区域中的样本认为属于同一个类别。常见的三种决策树分类模型有ID3、C4.5、CART，分别采用了信息增益、信息增益率、基尼系数的指标来进行节点的分裂。

在CART中，作者提出了基于决策树的回归方法，也即回归树，其基本思路与分类决策树相同，对于一棵树，由于我们的标签不再是离散型的类别而是连续型的数值，因此自上而下地按照**最小化误差平方和**的方法进行节点的分裂。具体来说，我们首先将数据集下的所有所有样本按照标签从小到大排序，便历切分点，每个切分点将原有数据集分裂成两个子类，对于每一个子类，采用样本均值来表示该节点下所有样本的预测值（最小化误差平方和的解），计算两个子类的误差平方和总和。选取误差平方和最小的切分点。接着，计算每个类别中样本标签值与预测值的残差，输入下一层，继续进行分裂。最后的预测值为每一层预测值相加。

由于我们将每一个叶子节点上的所有样本的预测值设置为一样的，因此相当于使用一个分段函数来拟合任意一条曲线。当分段点足够多时，理论上分段函数能够拟合任意一条光滑曲线。

**树模型的优缺点**

- 优点：解释性强，对缺失值、异常值不敏感，效率高
- 缺点：容易过拟合，不支持在线学习